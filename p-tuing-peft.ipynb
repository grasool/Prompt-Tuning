{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "model_name = \"bigscience/bloomz-560m\"\n",
    "NUM_VIRTUAL_TOKENS = 4\n",
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/peremartra/Large-Language-Model-Notebooks-Course/blob/main/5-Fine%20Tuning/Prompt_Tuning_PEFT.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "foundational_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_outputs(model, inputs, max_length=100):\n",
    "    outputs = model.generate(\n",
    "        inputs['input_ids'],\n",
    "        attention_mask=inputs['attention_mask'],\n",
    "        max_length=max_length,\n",
    "        repetition_penalty=1.5,\n",
    "        early_stopping=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    print(outputs)  # Add this line\n",
    "    return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  473,  4026,  1152,   427,  1769,   661,   267, 22675,   278,  4001,\n",
      "         67604,   613,  1074,     2]])\n",
      "[' I want you to act as a motivatinal coach for me']\n"
     ]
    }
   ],
   "source": [
    "input_prompt = tokenizer(\" I want you to act as a motivatinal coach for me\", return_tensors=\"pt\")\n",
    "foundational_outputs_prompt = get_outputs(foundational_model, input_prompt, max_length=50)\n",
    "print(tokenizer.batch_decode(foundational_outputs_prompt, skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_prompt = \"fka/awesome-chatgpt-prompts\"\n",
    "data_prompt = load_dataset(dataset_prompt)\n",
    "data_prompt = data_prompt.map(lambda samples:tokenizer(samples[\"prompt\"]), batched=True)\n",
    "train_sample_prompt = data_prompt[\"train\"].select(range(50))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote 1: I want you to act as a linux terminal. I will type commands and you will reply with what the terminal should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is pwd \n",
      "\n",
      "\n",
      "\n",
      "Quote 2: I want you to act as an English translator, spelling corrector and improver. I will speak to you in any language and you will detect the language, translate it and answer in the corrected and improved version of my text, in English. I want you to replace my simplified A0-level words and sentences with more beautiful and elegant, upper level English words and sentences. Keep the meaning same, but make them more literary. I want you to only reply the correction, the improvements and nothing else, do not write explanations. My first sentence is \"istanbulu cok seviyom burada olmak cok guzel\" \n",
      "\n",
      "\n",
      "\n",
      "Quote 3: I want you to act as an interviewer. I will be the candidate and you will ask me the interview questions for the `position` position. I want you to only reply as the interviewer. Do not write all the conservation at once. I want you to only do the interview with me. Ask me the questions and wait for my answers. Do not write explanations. Ask me the questions one by one like an interviewer does and wait for my answers. My first sentence is \"Hi\" \n",
      "\n",
      "\n",
      "\n",
      "Quote 4: I want you to act as a javascript console. I will type commands and you will reply with what the javascript console should show. I want you to only reply with the terminal output inside one unique code block, and nothing else. do not write explanations. do not type commands unless I instruct you to do so. when i need to tell you something in english, i will do so by putting text inside curly brackets {like this}. my first command is console.log(\"Hello World\"); \n",
      "\n",
      "\n",
      "\n",
      "Quote 5: I want you to act as a text based excel. you'll only reply me the text-based 10 rows excel sheet with row numbers and cell letters as columns (A to L). First column header should be empty to reference row number. I will tell you what to write into cells and you'll reply only the result of excel table as text, and nothing else. Do not write explanations. i will write you formulas and you'll execute formulas and you'll only reply the result of excel table as text. First, reply me the empty sheet. \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, sample in enumerate(train_sample_prompt['prompt'][:5]):\n",
    "    print(f'Quote {i+1}: {sample} \\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote 1: Need somebody with expertise on automobiles regarding troubleshooting solutions like; diagnosing problems/errors present both visually & within engine parts in order to figure out what's causing them (like lack of oil or power issues) & suggest required replacements while recording down details such fuel consumption type etc., First inquiry – “Car won't start although battery is full charged” \n",
      "\n",
      "\n",
      "\n",
      "Quote 2: I want you to act as an accountant and come up with creative ways to manage finances. You'll need to consider budgeting, investment strategies and risk management when creating a financial plan for your client. In some cases, you may also need to provide advice on taxation laws and regulations in order to help them maximize their profits. My first suggestion request is “Create a financial plan for a small business that focuses on cost savings and long-term investments\". \n",
      "\n",
      "\n",
      "\n",
      "Quote 3: I want you to act as a real estate agent. I will provide you with details on an individual looking for their dream home, and your role is to help them find the perfect property based on their budget, lifestyle preferences, location requirements etc. You should use your knowledge of the local housing market in order to suggest properties that fit all the criteria provided by the client. My first request is \"I need help finding a single story family house near downtown Istanbul.\" \n",
      "\n",
      "\n",
      "\n",
      "Quote 4: I want you to act as a etymologist. I will give you a word and you will research the origin of that word, tracing it back to its ancient roots. You should also provide information on how the meaning of the word has changed over time, if applicable. My first request is \"I want to trace the origins of the word 'pizza'.\" \n",
      "\n",
      "\n",
      "\n",
      "Quote 5: I want you to act as a doctor and come up with creative treatments for illnesses or diseases. You should be able to recommend conventional medicines, herbal remedies and other natural alternatives. You will also need to consider the patient’s age, lifestyle and medical history when providing your recommendations. My first suggestion request is “Come up with a treatment plan that focuses on holistic healing methods for an elderly patient suffering from arthritis\". \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "samples = random.sample(train_sample_prompt['prompt'], 5)\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f'Quote {i+1}: {sample} \\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_sentences = load_dataset(\"Abirate/english_quotes\")\n",
    "data_sentences = dataset_sentences.map(lambda samples: tokenizer(samples[\"quote\"]), batched=True)\n",
    "train_sample_sentences = data_sentences[\"train\"].select(range(25))\n",
    "train_sample_sentences = train_sample_sentences.remove_columns(['author', 'tags'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quote 1: I want you to act as a philosophy teacher. I will provide some topics related to the study of philosophy, and it will be your job to explain these concepts in an easy-to-understand manner. This could include providing examples, posing questions or breaking down complex ideas into smaller pieces that are easier to comprehend. My first request is \"I need help understanding how different philosophical theories can be applied in everyday life.\" \n",
      "\n",
      "\n",
      "\n",
      "Quote 2: I want you to act as a career counselor. I will provide you with an individual looking for guidance in their professional life, and your task is to help them determine what careers they are most suited for based on their skills, interests and experience. You should also conduct research into the various options available, explain the job market trends in different industries and advice on which qualifications would be beneficial for pursuing particular fields. My first request is \"I want to advise someone who wants to pursue a potential career in software engineering.\" \n",
      "\n",
      "\n",
      "\n",
      "Quote 3: I want you to act as a web design consultant. I will provide you with details related to an organization needing assistance designing or redeveloping their website, and your role is to suggest the most suitable interface and features that can enhance user experience while also meeting the company's business goals. You should use your knowledge of UX/UI design principles, coding languages, website development tools etc., in order to develop a comprehensive plan for the project. My first request is \"I need help creating an e-commerce site for selling jewelry.\" \n",
      "\n",
      "\n",
      "\n",
      "Quote 4: I want you to act as an artist advisor providing advice on various art styles such tips on utilizing light & shadow effects effectively in painting, shading techniques while sculpting etc., Also suggest music piece that could accompany artwork nicely depending upon its genre/style type along with appropriate reference images demonstrating your recommendations regarding same; all this in order help out aspiring artists explore new creative possibilities & practice ideas which will further help them sharpen their skills accordingly! First request - “I’m making surrealistic portrait paintings” \n",
      "\n",
      "\n",
      "\n",
      "Quote 5: I require someone who can suggest delicious recipes that includes foods which are nutritionally beneficial but also easy & not time consuming enough therefore suitable for busy people like us among other factors such as cost effectiveness so overall dish ends up being healthy yet economical at same time! My first request – “Something light yet fulfilling that could be cooked quickly during lunch break” \n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "samples = random.sample(train_sample_prompt['prompt'], 5)\n",
    "for i, sample in enumerate(samples):\n",
    "    print(f'Quote {i+1}: {sample} \\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import  get_peft_model, PromptTuningConfig, TaskType, PromptTuningInit\n",
    "\n",
    "generation_config = PromptTuningConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, #This type indicates the model will generate text.\n",
    "    prompt_tuning_init=PromptTuningInit.RANDOM,  #The added virtual tokens are initializad with random numbers\n",
    "    num_virtual_tokens=NUM_VIRTUAL_TOKENS, #Number of virtual tokens to be added and trained.\n",
    "    tokenizer_name_or_path=model_name #The pre-trained model.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "peft_model_prompt = get_peft_model(foundational_model, generation_config)\n",
    "print(peft_model_prompt.print_trainable_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 4,096 || all params: 559,218,688 || trainable%: 0.0007324504863471229\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "peft_model_sentences = get_peft_model(foundational_model, generation_config)\n",
    "print(peft_model_sentences.print_trainable_parameters())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "def create_training_arguments(path, learning_rate=0.0035, epochs=6):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=path, # Where the model predictions and checkpoints will be written\n",
    "        use_cpu=True, # This is necessary for CPU clusters.\n",
    "        auto_find_batch_size=True, # Find a suitable batch size that will fit into memory automatically\n",
    "        learning_rate= learning_rate, # Higher learning rate than full fine-tuning\n",
    "        num_train_epochs=epochs\n",
    "    )\n",
    "    return training_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "working_dir = \"./\"\n",
    "\n",
    "#Is best to store the models in separate folders.\n",
    "#Create the name of the directories where to store the models.\n",
    "output_directory_prompt =  os.path.join(working_dir, \"peft_outputs_prompt\")\n",
    "output_directory_sentences = os.path.join(working_dir, \"peft_outputs_sentences\")\n",
    "\n",
    "#Just creating the directoris if not exist.\n",
    "if not os.path.exists(working_dir):\n",
    "    os.mkdir(working_dir)\n",
    "if not os.path.exists(output_directory_prompt):\n",
    "    os.mkdir(output_directory_prompt)\n",
    "if not os.path.exists(output_directory_sentences):\n",
    "    os.mkdir(output_directory_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_prompt = create_training_arguments(output_directory_prompt, 0.003, NUM_EPOCHS)\n",
    "training_args_sentences = create_training_arguments(output_directory_sentences, 0.003, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, DataCollatorForLanguageModeling\n",
    "def create_trainer(model, training_args, train_dataset):\n",
    "    trainer = Trainer(\n",
    "        model=model, # We pass in the PEFT version of the foundation model, bloomz-560M\n",
    "        args=training_args, #The args for the training.\n",
    "        train_dataset=train_dataset, #The dataset used to tyrain the model.\n",
    "        data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False) # mlm=False indicates not to use masked language modeling\n",
    "    )\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1205f484fd3493997854c8ecfad8d71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 177.8568, 'train_samples_per_second': 1.406, 'train_steps_per_second': 0.197, 'train_loss': 3.592269025530134, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=35, training_loss=3.592269025530134, metrics={'train_runtime': 177.8568, 'train_samples_per_second': 1.406, 'train_steps_per_second': 0.197, 'train_loss': 3.592269025530134, 'epoch': 5.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training first model.\n",
    "trainer_prompt = create_trainer(peft_model_prompt, training_args_prompt, train_sample_prompt)\n",
    "trainer_prompt.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f50144d3a1b4a98b93bf61baca37335",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 53.3114, 'train_samples_per_second': 2.345, 'train_steps_per_second': 0.375, 'train_loss': 4.46654052734375, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=4.46654052734375, metrics={'train_runtime': 53.3114, 'train_samples_per_second': 2.345, 'train_steps_per_second': 0.375, 'train_loss': 4.46654052734375, 'epoch': 5.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Training second model.\n",
    "trainer_sentences = create_trainer(peft_model_sentences, training_args_sentences, train_sample_sentences)\n",
    "trainer_sentences.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_prompt.model.save_pretrained(output_directory_prompt)\n",
    "trainer_sentences.model.save_pretrained(output_directory_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "loaded_model_prompt = PeftModel.from_pretrained(foundational_model,\n",
    "                                         output_directory_prompt,\n",
    "                                         #device_map='auto',\n",
    "                                         is_trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  473,  4026,  1152,   427,  1769,   661,   267, 22675,   278,  4001,\n",
      "         67604,   613,  1074,     2]])\n",
      "[' I want you to act as a motivatinal coach for me']\n"
     ]
    }
   ],
   "source": [
    "loaded_model_prompt_outputs = get_outputs(loaded_model_prompt, input_prompt)\n",
    "print(tokenizer.batch_decode(loaded_model_prompt_outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-testingp310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
