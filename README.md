# Prompt Tuning
These are some scripts that I am using to try promt tuning.


## Why prompt tuning is still relevant when we can do "prompt engineering" with very long contexts. 

The Mistral 7B model that I am using has a context length of 32768 tokens. should I still do prompt tuning?

I am not sure and trying to figure this out.

